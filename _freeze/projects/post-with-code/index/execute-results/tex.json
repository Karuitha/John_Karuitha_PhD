{
  "hash": "1a6b72b955325885471f69136ce63ae9",
  "result": {
    "markdown": "---\n# title: \"Post With Code\"\n# author: \"John Karuitha\"\n# date: \"2020-01-27\"\ncategories: [news, code, analysis]\nimage: \"banner.jpg\"\ntitle: \"**Predicting Wine Quality Using Python: Support Vector Machines, Random Forest and Neural Networks**\"\ndescription: \"Independent Data Analysis Project\"\nauthor: \n    - name: John Karuitha\n      url: www.linkedin.com/in/Karuitha\n      affiliation: \"Karatina University, School of Business, Karatina, Kenya\"\n      affiliation-url: www.karu.ac.ke\nsubtitle: \"*Applied Machine Learning in sklearn*\"\ndate: today\ntitle-block-banner: \"banner.jpg\"\ntitle-block-banner-color: \"white\"\nformat: \n    html:\n        theme: sandstone\n        number-sections: true\n        code-fold: false\n        code-background: true\n        toc: true\n        toc-title: \"Contents\"\n        toc-depth: 3\n        toc-float: true\n        linkcolor: \"gray\"\n        link-citations: true\n    pdf:\n        header-includes: |\n            \\usepackage{pdflscape}\n            \\usepackage[OT1]{fontenc}\n            \\newcommand{\\blandscape}{\\begin{landscape}}\n            \\newcommand{\\elandscape}{\\end{landscape}}\n        toc: true\n        toc-title: \"Contents\"\n        toc-depth: 3\n        toccolor: \"blue\"\n        number-sections: true\n        number-depth: 3\n        documentclass: report\n        margin-left: 30mm\n        margin-right: 30mm\n        linkcolor: \"blue\"\n        link-citations: true\neditor: visual\nbibliography: citations.bib\ncsl: harvard.csl\n---\n\n# **Background**\n\nIn this analysis, I use data from the UCI machine learning repository regarding the quality of red wine. The objective is to develop machine learning algorithms that can reasonably predict the wine rating based on a set of variables/features. Specifically, we train 3 machine learning models.\n\n1.  Support vector machines (SVM) [@pisner2020support].\n2.  Random forest model [@parmar2019review].\n3.  Neural network model [@hancock2020survey].\n\n\n::: callout-tip\n## Read More of my Work\n\nPlease visit [my rpubs site](www.rpubs.com/Karuitha) to see more data projects. Alternatively, copy and paste the link <https://www.rpubs.com/Karuitha> into your browser.\n\nMy data visualizations projects are available in my [Tableau Public profile page](https://public.tableau.com/app/profile/john.karuitha) or copy and paste the link <https://public.tableau.com/app/profile/john.karuitha>.\n\nMy Shiny web apps are available on this [site](https://karuitha.shinyapps.io/). You can copy-paste this web address instead <https://karuitha.shinyapps.io/>.\n:::\n\n::: callout-note\n## Tools Utilized & Skills Applied\n\nPython, sklearn, matplotlib, pandas, seaborn, numpy, Data Science, Machine Learning\n:::\n\n\nThe data that we utilise in this analysis is available on this [link](https://archive.ics.uci.edu/ml/datasets/wine+quality). For the purpose of this analysis, I use data for red wine. [^1]\n\n[^1]: Note: The data is also available for white wine.See <https://archive.ics.uci.edu/ml/datasets/wine+quality>.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.utils import resample\n```\n:::\n\n\nI start by downloading and reading the dataset.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# help(pd.read_csv)\nwine = pd.read_csv(\"winequality-red.csv\", sep = \";\")\nwine.head()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/karuitha/anaconda3/envs/py3108/lib/python3.10/site-packages/IPython/core/formatters.py:345: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=26}\n```{=tex}\n\\begin{tabular}{lrrrrrrrrrrrr}\n\\toprule\n{} &  fixed acidity &  volatile acidity &  citric acid &  residual sugar &  chlorides &  free sulfur dioxide &  total sulfur dioxide &  density &    pH &  sulphates &  alcohol &  quality \\\\\n\\midrule\n0 &            7.4 &              0.70 &         0.00 &             1.9 &      0.076 &                 11.0 &                  34.0 &   0.9978 &  3.51 &       0.56 &      9.4 &        5 \\\\\n1 &            7.8 &              0.88 &         0.00 &             2.6 &      0.098 &                 25.0 &                  67.0 &   0.9968 &  3.20 &       0.68 &      9.8 &        5 \\\\\n2 &            7.8 &              0.76 &         0.04 &             2.3 &      0.092 &                 15.0 &                  54.0 &   0.9970 &  3.26 &       0.65 &      9.8 &        5 \\\\\n3 &           11.2 &              0.28 &         0.56 &             1.9 &      0.075 &                 17.0 &                  60.0 &   0.9980 &  3.16 &       0.58 &      9.8 &        6 \\\\\n4 &            7.4 &              0.70 &         0.00 &             1.9 &      0.076 &                 11.0 &                  34.0 &   0.9978 &  3.51 &       0.56 &      9.4 &        5 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\nNext, we look at the number of rows and columns in the dataset.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nwine.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n(1599, 12)\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nwine.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1599 entries, 0 to 1598\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         1599 non-null   float64\n 1   volatile acidity      1599 non-null   float64\n 2   citric acid           1599 non-null   float64\n 3   residual sugar        1599 non-null   float64\n 4   chlorides             1599 non-null   float64\n 5   free sulfur dioxide   1599 non-null   float64\n 6   total sulfur dioxide  1599 non-null   float64\n 7   density               1599 non-null   float64\n 8   pH                    1599 non-null   float64\n 9   sulphates             1599 non-null   float64\n 10  alcohol               1599 non-null   float64\n 11  quality               1599 non-null   int64  \ndtypes: float64(11), int64(1)\nmemory usage: 150.0 KB\n```\n:::\n:::\n\n\n# **Explore the Data**\n\nIn this section, we examine the data. First, we list the variables.\n\nThe input variables (based on physicochemical tests):\n\n1.  fixed acidity\n2.  volatile acidity\n3.  citric acid\n4.  residual sugar\n5.  chlorides\n6.  free sulfur dioxide\n7.  total sulfur dioxide\n8.  density\n9.  pH\n10. sulphates\n11. alcohol\n\nThe output variable (based on sensory data):\n\n1.  quality (score between 0 and 10)\n\nWe then check for missing and duplicate values. We see that there are no missing values.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nwine.isna().sum()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/karuitha/anaconda3/envs/py3108/lib/python3.10/site-packages/IPython/core/formatters.py:345: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=tex}\n\\begin{tabular}{lr}\n\\toprule\n{} &  0 \\\\\n\\midrule\nfixed acidity        &  0 \\\\\nvolatile acidity     &  0 \\\\\ncitric acid          &  0 \\\\\nresidual sugar       &  0 \\\\\nchlorides            &  0 \\\\\nfree sulfur dioxide  &  0 \\\\\ntotal sulfur dioxide &  0 \\\\\ndensity              &  0 \\\\\npH                   &  0 \\\\\nsulphates            &  0 \\\\\nalcohol              &  0 \\\\\nquality              &  0 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\nHowever, there are 240 duplicated observations that we drop.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nwine.duplicated().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n240\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nwine = wine.drop_duplicates()\n```\n:::\n\n\nNext, I do feature engineering by converting the target variable `quality` to a binary variable. As it stands, the wine is in the following categories.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nwine[\"quality\"].unique()\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\narray([5, 6, 7, 4, 8, 3])\n```\n:::\n:::\n\n\nSpecifically, wines below a rating of () are bad quality while those equal to or above () are good quality wines. Note that this is a personal choice and hence subjective.\n\nNote that after this update, we only have 2 categories for wine quality, 0 and 1.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nwine[\"quality\"] = [1 if i >= 6.5 else 0 for i in wine[\"quality\"]]\n\nwine.head()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/karuitha/anaconda3/envs/py3108/lib/python3.10/site-packages/IPython/core/formatters.py:345: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=tex}\n\\begin{tabular}{lrrrrrrrrrrrr}\n\\toprule\n{} &  fixed acidity &  volatile acidity &  citric acid &  residual sugar &  chlorides &  free sulfur dioxide &  total sulfur dioxide &  density &    pH &  sulphates &  alcohol &  quality \\\\\n\\midrule\n0 &            7.4 &              0.70 &         0.00 &             1.9 &      0.076 &                 11.0 &                  34.0 &   0.9978 &  3.51 &       0.56 &      9.4 &        0 \\\\\n1 &            7.8 &              0.88 &         0.00 &             2.6 &      0.098 &                 25.0 &                  67.0 &   0.9968 &  3.20 &       0.68 &      9.8 &        0 \\\\\n2 &            7.8 &              0.76 &         0.04 &             2.3 &      0.092 &                 15.0 &                  54.0 &   0.9970 &  3.26 &       0.65 &      9.8 &        0 \\\\\n3 &           11.2 &              0.28 &         0.56 &             1.9 &      0.075 &                 17.0 &                  60.0 &   0.9980 &  3.16 &       0.58 &      9.8 &        0 \\\\\n5 &            7.4 &              0.66 &         0.00 &             1.8 &      0.075 &                 13.0 &                  40.0 &   0.9978 &  3.51 &       0.56 &      9.4 &        0 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n# **Data Visualization**\n\nIn this section, I visualize the data. To start with, I make a histogram for indepedent variables/ features.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# plt.subplots(4, 3, figsize=(12, 8))\nfor variable in wine.columns[:-1]:\n\n    plt.hist(wine[wine[\"quality\"] == 1][variable], color = \"purple\", alpha = 0.5, density = True)\n\n    plt.hist(wine[wine[\"quality\"] == 0][variable], color = \"green\", alpha = 0.5, density = True)\n\n    plt.title(variable)\n    plt.xlabel(variable)\n    plt.ylabel(\"Probability\")\n    plt.legend()\n\n    \n    plt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-4.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-6.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-8.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-10.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-12.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-14.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-16.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-18.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-20.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-11-output-22.pdf){fig-pos='H'}\n:::\n:::\n\n\nOverall, it appears like Alcohol content is a great disciminator for the quality of wines although the other variables are also useful.\n\n## **Evaluating Class Balance/ Imbalance**\n\nIn this section, we evaluate the class balance in the dataset. As shown below, there is a high degree of the class balance with 217 good wines in the dataset against 1382 bad wines. This level of the class balance could affect the effeciency of the machine learning models. One approach is to upsample the underrepresented class or downsample the overrepresented class. This approach is more cost efficient. The alternative approach is is to get more data for the underrepresented class.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nwine[\"quality\"].value_counts()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/karuitha/anaconda3/envs/py3108/lib/python3.10/site-packages/IPython/core/formatters.py:345: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=35}\n```{=tex}\n\\begin{tabular}{lr}\n\\toprule\n{} &  quality \\\\\n\\midrule\n0 &     1175 \\\\\n1 &      184 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# help(sns.countplot)\nsns.countplot(wine[\"quality\"], palette = \"dark\")\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\n<AxesSubplot: ylabel='count'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-13-output-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe shall upsample the training data later.\n\n# **Baseline Evaluation Metric**\n\nTo evaluate whether our models work well, we have to develop the baseline evaluation metric. We pose this question; if a person were to guess that the quality of all wines is bad (remember bad is the dominant class in the data), what would be their accuracy?\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nwine[\"quality\"].value_counts()\n\n1382 / (1382 + 217)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\n0.8642901813633521\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n[0]*1000\nconfusion_matrix(wine[\"quality\"], [0] * len(wine))\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\narray([[1175,    0],\n       [ 184,    0]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nclassification_report(wine[\"quality\"], [0] * len(wine))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/karuitha/anaconda3/envs/py3108/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n\nPrecision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n/home/karuitha/anaconda3/envs/py3108/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n\nPrecision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n/home/karuitha/anaconda3/envs/py3108/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning:\n\nPrecision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\n'              precision    recall  f1-score   support\\n\\n           0       0.86      1.00      0.93      1175\\n           1       0.00      0.00      0.00       184\\n\\n    accuracy                           0.86      1359\\n   macro avg       0.43      0.50      0.46      1359\\nweighted avg       0.75      0.86      0.80      1359\\n'\n```\n:::\n:::\n\n\nIn this case, the person would be 86% accurate. Hence, our models have to be more than 86% accurate. For the other scores like precision and recall, we ought to do better than this baseline.\n\n# **Training and Testing Sets**\n\nIn this section we will create a training set and a test set. We set aside 20% of the data for testing and use the remainder for testing.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nx = wine.drop(columns = [\"quality\"])\ny = wine[\"quality\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n\nx_train.head()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/karuitha/anaconda3/envs/py3108/lib/python3.10/site-packages/IPython/core/formatters.py:345: FutureWarning:\n\nIn future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=40}\n```{=tex}\n\\begin{tabular}{lrrrrrrrrrrr}\n\\toprule\n{} &  fixed acidity &  volatile acidity &  citric acid &  residual sugar &  chlorides &  free sulfur dioxide &  total sulfur dioxide &  density &    pH &  sulphates &  alcohol \\\\\n\\midrule\n3    &           11.2 &             0.280 &         0.56 &             1.9 &      0.075 &                 17.0 &                  60.0 &  0.99800 &  3.16 &       0.58 &      9.8 \\\\\n1462 &            6.8 &             0.640 &         0.03 &             2.3 &      0.075 &                 14.0 &                  31.0 &  0.99545 &  3.36 &       0.58 &     10.4 \\\\\n576  &            9.9 &             0.500 &         0.24 &             2.3 &      0.103 &                  6.0 &                  14.0 &  0.99780 &  3.34 &       0.52 &     10.0 \\\\\n45   &            4.6 &             0.520 &         0.15 &             2.1 &      0.054 &                  8.0 &                  65.0 &  0.99340 &  3.90 &       0.56 &     13.1 \\\\\n1352 &            7.6 &             0.645 &         0.03 &             1.9 &      0.086 &                 14.0 &                  57.0 &  0.99690 &  3.37 &       0.46 &     10.3 \\\\\n\\bottomrule\n\\end{tabular}\n```\n:::\n:::\n\n\n# **Scaling the Data**\n\nThe scale of the data could also affect the performance of the machine learning models. For instance, if one variable is in the millions (e.g. 5,233,150) while another is a fraction (e.g. 0.5), the models will likely pick the signal in the larger value more than the fraction. However, the fraction could also contain a valuable signal that is masked due to the scale.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\nx_train[:3]\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\narray([[ 1.65650881, -1.38004607,  1.45916152, -0.44870117, -0.26606052,\n         0.1113736 ,  0.37699549,  0.68484681, -0.95193783, -0.44930832,\n        -0.57838334],\n       [-0.85880973,  0.62805874, -1.24411385, -0.16971466, -0.26606052,\n        -0.17258989, -0.46366724, -0.6609637 ,  0.32059051, -0.44930832,\n        -0.03276604],\n       [ 0.91334651, -0.1528709 , -0.17300474, -0.16971466,  0.30424851,\n        -0.92982588, -0.95646954,  0.57929305,  0.19333768, -0.7943441 ,\n        -0.39651091]])\n```\n:::\n:::\n\n\nNow our data is in the same scale.\n\n# **Training the Models**\n\nIn this section we train the three models in the following order:\n\n-   Support vector machines model.\n-   Random forest model.\n-   Neural network model.\n\n## **Support Vector Machines**\n\nIn this section, we train the support vector machines model.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nclf = svm.SVC()\nclf.fit(x_train, y_train)\n\nclf_predictions = clf.predict(x_test)\n\nconfusion_matrix(y_test, clf_predictions)\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\narray([[234,   4],\n       [ 22,  12]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nclassification_report(y_test, clf_predictions)\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\n'              precision    recall  f1-score   support\\n\\n           0       0.91      0.98      0.95       238\\n           1       0.75      0.35      0.48        34\\n\\n    accuracy                           0.90       272\\n   macro avg       0.83      0.67      0.71       272\\nweighted avg       0.89      0.90      0.89       272\\n'\n```\n:::\n:::\n\n\nAgain this model does better than the baseline model.\n\n## **Random Forest Model**\n\nNext, we fit the random forest model with 1000 trees in the forest.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nrf_model = RandomForestClassifier(n_estimators=1000)\n\nrf_model.fit(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\nRandomForestClassifier(n_estimators=1000)\n```\n:::\n:::\n\n\nWe examine the metrics of the model, that is how it performs in the testing set. We start by doing the prediction using the model and then evaluate the performance of the model on the testing set. In this case, we have an accuracy of 94%, sensitivity at 98%, and specificity at 96%. This is above the base metrics in section 4.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\npredictions_rfm = rf_model.predict(x_test)\n\nclassification_report(y_test, predictions_rfm)\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\n'              precision    recall  f1-score   support\\n\\n           0       0.93      0.97      0.95       238\\n           1       0.67      0.47      0.55        34\\n\\n    accuracy                           0.90       272\\n   macro avg       0.80      0.72      0.75       272\\nweighted avg       0.89      0.90      0.90       272\\n'\n```\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nconfusion_matrix(y_test, predictions_rfm)\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\narray([[230,   8],\n       [ 18,  16]])\n```\n:::\n:::\n\n\n## **Neural Network Model**\n\nNeural nets work well with huge amounts of data, more so text, images and other unstructured data. The accuracy of 89% is marginally above the baseline metric of 86% in section 4.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nmlp = MLPClassifier(hidden_layer_sizes = (11, 11, 11), max_iter = 1000)\n\nmlp.fit(x_train, y_train)\n\nmlp_predictions = mlp.predict(x_test)\n\nconfusion_matrix(y_test, mlp_predictions)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/karuitha/anaconda3/envs/py3108/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning:\n\nStochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\narray([[225,  13],\n       [ 18,  16]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nclassification_report(y_test, mlp_predictions)\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\n'              precision    recall  f1-score   support\\n\\n           0       0.93      0.95      0.94       238\\n           1       0.55      0.47      0.51        34\\n\\n    accuracy                           0.89       272\\n   macro avg       0.74      0.71      0.72       272\\nweighted avg       0.88      0.89      0.88       272\\n'\n```\n:::\n:::\n\n\n# **Conclusion**\n\nIn this analysis, we have trained the following classification models to predict the quality of red wine.\n\n-   Support vector machines model.\n-   Random forest model.\n-   Neural network model.\n\nThe baseline accuracy was 86% and the all the models seem to outperform this baseline accuracy. The random forest model does better than the neural network model and the support vector machine model. The models could be improved through hyperparameter tuning and the upsampling or downsampling of the underrepresented class in the dependent or outcome variable.\n\n# **References** {.unnumbered}\n\n",
    "supporting": [
      "index_files/figure-pdf"
    ],
    "filters": []
  }
}